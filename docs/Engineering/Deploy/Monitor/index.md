# åº”ç”¨ç›‘æ§ç³»ç»Ÿ

## ç®€ä»‹
### ğŸ“Š ä»€ä¹ˆæ˜¯åº”ç”¨ç›‘æ§

åº”ç”¨ç›‘æ§æ˜¯å¯¹è½¯ä»¶ç³»ç»Ÿè¿è¡ŒçŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡ã€é”™è¯¯æ—¥å¿—ç­‰è¿›è¡Œå®æ—¶è§‚æµ‹å’Œåˆ†æçš„è¿‡ç¨‹ï¼Œæ˜¯ä¿éšœç³»ç»Ÿç¨³å®šè¿è¡Œçš„é‡è¦æ‰‹æ®µã€‚

### ç›‘æ§ä½“ç³»
- **åŸºç¡€è®¾æ–½ç›‘æ§**ï¼šæœåŠ¡å™¨ã€ç½‘ç»œã€å­˜å‚¨
- **åº”ç”¨æ€§èƒ½ç›‘æ§ï¼ˆAPMï¼‰**ï¼šå“åº”æ—¶é—´ã€ååé‡ã€é”™è¯¯ç‡
- **æ—¥å¿—ç›‘æ§**ï¼šé”™è¯¯æ—¥å¿—ã€è®¿é—®æ—¥å¿—ã€ä¸šåŠ¡æ—¥å¿—
- **ç”¨æˆ·ä½“éªŒç›‘æ§ï¼ˆRUMï¼‰**ï¼šé¡µé¢åŠ è½½ã€ç”¨æˆ·è¡Œä¸º
- **ä¸šåŠ¡ç›‘æ§**ï¼šæ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡ã€è½¬åŒ–ç‡

### ç›‘æ§æŒ‡æ ‡ï¼ˆå››ä¸ªé»„é‡‘ä¿¡å·ï¼‰
- **å»¶è¿Ÿï¼ˆLatencyï¼‰**ï¼šè¯·æ±‚å“åº”æ—¶é—´
- **æµé‡ï¼ˆTrafficï¼‰**ï¼šç³»ç»Ÿå¤„ç†çš„è¯·æ±‚é‡
- **é”™è¯¯ï¼ˆErrorsï¼‰**ï¼šå¤±è´¥è¯·æ±‚çš„æ¯”ç‡
- **é¥±å’Œåº¦ï¼ˆSaturationï¼‰**ï¼šç³»ç»Ÿèµ„æºä½¿ç”¨ç‡

## Prometheus + Grafana
### Prometheus é…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # Prometheus è‡ªèº«ç›‘æ§
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Node Exporter ç³»ç»Ÿç›‘æ§
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  # åº”ç”¨ç›‘æ§
  - job_name: 'myapp'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  # Docker å®¹å™¨ç›‘æ§
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']

  # Redis ç›‘æ§
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # PostgreSQL ç›‘æ§
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  # Nginx ç›‘æ§
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']
```

### å‘Šè­¦è§„åˆ™é…ç½®

```yaml
# alert_rules.yml
groups:
- name: system_alerts
  rules:
  # CPU ä½¿ç”¨ç‡å‘Šè­¦
  - alert: HighCPUUsage
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage detected"
      description: "CPU usage is above 80% for more than 5 minutes on {{ $labels.instance }}"

  # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
  - alert: HighMemoryUsage
    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage detected"
      description: "Memory usage is above 85% on {{ $labels.instance }}"

  # ç£ç›˜ç©ºé—´å‘Šè­¦
  - alert: DiskSpaceLow
    expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Disk space is running low"
      description: "Disk space is below 10% on {{ $labels.instance }}"

- name: application_alerts
  rules:
  # åº”ç”¨å“åº”æ—¶é—´å‘Šè­¦
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High response time detected"
      description: "95th percentile response time is above 1 second"

  # é”™è¯¯ç‡å‘Šè­¦
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100 > 5
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "High error rate detected"
      description: "Error rate is above 5% for the last 5 minutes"

  # åº”ç”¨æœåŠ¡ä¸å¯ç”¨
  - alert: ServiceDown
    expr: up == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Service is down"
      description: "{{ $labels.job }} service is down on {{ $labels.instance }}"
```

### Grafana ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "Application Monitoring Dashboard",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{status}}"
          }
        ],
        "yAxes": [
          {
            "label": "Requests/sec"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "99th percentile"
          }
        ],
        "yAxes": [
          {
            "label": "Seconds"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "singlestat",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m]) * 100"
          }
        ],
        "format": "percent",
        "thresholds": "1,5"
      }
    ]
  }
}
```

## åº”ç”¨æ€§èƒ½ç›‘æ§ï¼ˆAPMï¼‰
### Node.js åº”ç”¨ç›‘æ§

```javascript
// app.js - Express åº”ç”¨ç›‘æ§
const express = require('express')
const prometheus = require('prom-client')
const responseTime = require('response-time')

const app = express()

// åˆ›å»º Prometheus æŒ‡æ ‡
const httpRequestDuration = new prometheus.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status'],
  buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10]
})

const httpRequestsTotal = new prometheus.Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status']
})

const activeConnections = new prometheus.Gauge({
  name: 'http_active_connections',
  help: 'Number of active HTTP connections'
})

// ä¸­é—´ä»¶ï¼šè®°å½•è¯·æ±‚æŒ‡æ ‡
app.use(responseTime((req, res, time) => {
  const route = req.route ? req.route.path : req.path
  const labels = {
    method: req.method,
    route: route,
    status: res.statusCode
  }
  
  httpRequestDuration.observe(labels, time / 1000)
  httpRequestsTotal.inc(labels)
}))

// ä¸­é—´ä»¶ï¼šè®°å½•æ´»è·ƒè¿æ¥æ•°
app.use((req, res, next) => {
  activeConnections.inc()
  res.on('finish', () => {
    activeConnections.dec()
  })
  next()
})

// å¥åº·æ£€æŸ¥ç«¯ç‚¹
app.get('/health', (req, res) => {
  res.json({ status: 'healthy', timestamp: new Date().toISOString() })
})

// æŒ‡æ ‡ç«¯ç‚¹
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', prometheus.register.contentType)
  res.end(await prometheus.register.metrics())
})

// ä¸šåŠ¡è·¯ç”±
app.get('/api/users', async (req, res) => {
  try {
    // æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
    const users = await getUsersFromDB()
    res.json(users)
  } catch (error) {
    console.error('Error fetching users:', error)
    res.status(500).json({ error: 'Internal server error' })
  }
})

app.listen(3000, () => {
  console.log('Server running on port 3000')
})
```

### Python åº”ç”¨ç›‘æ§

```python
# app.py - Flask åº”ç”¨ç›‘æ§
from flask import Flask, request, jsonify
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time
import logging

app = Flask(__name__)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Prometheus æŒ‡æ ‡
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

ACTIVE_REQUESTS = Gauge(
    'http_active_requests',
    'Active HTTP requests'
)

# è¯·æ±‚ç›‘æ§è£…é¥°å™¨
def monitor_requests(f):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        ACTIVE_REQUESTS.inc()
        
        try:
            response = f(*args, **kwargs)
            status = response.status_code if hasattr(response, 'status_code') else 200
        except Exception as e:
            status = 500
            logger.error(f"Request failed: {e}")
            raise
        finally:
            duration = time.time() - start_time
            ACTIVE_REQUESTS.dec()
            
            REQUEST_COUNT.labels(
                method=request.method,
                endpoint=request.endpoint or 'unknown',
                status=status
            ).inc()
            
            REQUEST_DURATION.labels(
                method=request.method,
                endpoint=request.endpoint or 'unknown'
            ).observe(duration)
        
        return response
    
    wrapper.__name__ = f.__name__
    return wrapper

@app.route('/health')
@monitor_requests
def health_check():
    return jsonify({
        'status': 'healthy',
        'timestamp': time.time()
    })

@app.route('/metrics')
def metrics():
    return generate_latest()

@app.route('/api/users')
@monitor_requests
def get_users():
    try:
        # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
        users = fetch_users_from_db()
        return jsonify(users)
    except Exception as e:
        logger.error(f"Error fetching users: {e}")
        return jsonify({'error': 'Internal server error'}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

## æ—¥å¿—ç›‘æ§
### ELK Stack é…ç½®

```yaml
# docker-compose.elk.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      - elk

  logstash:
    image: docker.elastic.co/logstash/logstash:8.5.0
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./logstash/config:/usr/share/logstash/config:ro
    ports:
      - "5044:5044"
      - "9600:9600"
    environment:
      - "LS_JAVA_OPTS=-Xmx512m -Xms512m"
    networks:
      - elk
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.5.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    networks:
      - elk
    depends_on:
      - elasticsearch

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.5.0
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/log:/var/log:ro
    networks:
      - elk
    depends_on:
      - logstash

volumes:
  elasticsearch_data:

networks:
  elk:
    driver: bridge
```

### Logstash é…ç½®

```ruby
# logstash/pipeline/logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  # è§£æ JSON æ—¥å¿—
  if [fields][log_type] == "application" {
    json {
      source => "message"
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    mutate {
      add_field => { "log_level" => "%{level}" }
    }
  }
  
  # è§£æ Nginx è®¿é—®æ—¥å¿—
  if [fields][log_type] == "nginx" {
    grok {
      match => { 
        "message" => "%{COMBINEDAPACHELOG}"
      }
    }
    
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    mutate {
      convert => { "response" => "integer" }
      convert => { "bytes" => "integer" }
    }
  }
  
  # æ·»åŠ åœ°ç†ä½ç½®ä¿¡æ¯
  if [clientip] {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # è°ƒè¯•è¾“å‡º
  stdout {
    codec => rubydebug
  }
}
```

### ç»“æ„åŒ–æ—¥å¿—

```javascript
// logger.js - ç»“æ„åŒ–æ—¥å¿—é…ç½®
const winston = require('winston')
const { ElasticsearchTransport } = require('winston-elasticsearch')

const esTransportOpts = {
  level: 'info',
  clientOpts: {
    node: 'http://elasticsearch:9200'
  },
  index: 'app-logs',
  transformer: (logData) => {
    return {
      '@timestamp': new Date().toISOString(),
      level: logData.level,
      message: logData.message,
      service: 'myapp',
      environment: process.env.NODE_ENV || 'development',
      ...logData.meta
    }
  }
}

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  defaultMeta: {
    service: 'myapp',
    version: process.env.APP_VERSION || '1.0.0'
  },
  transports: [
    new winston.transports.File({ filename: 'logs/error.log', level: 'error' }),
    new winston.transports.File({ filename: 'logs/combined.log' }),
    new winston.transports.Console({
      format: winston.format.simple()
    }),
    new ElasticsearchTransport(esTransportOpts)
  ]
})

// ä½¿ç”¨ç¤ºä¾‹
logger.info('User login', {
  userId: '12345',
  email: 'user@example.com',
  ip: '192.168.1.1',
  userAgent: 'Mozilla/5.0...'
})

logger.error('Database connection failed', {
  error: error.message,
  stack: error.stack,
  database: 'postgresql',
  host: 'db.example.com'
})

module.exports = logger
```

## å‰ç«¯ç›‘æ§
### ç”¨æˆ·ä½“éªŒç›‘æ§

```javascript
// frontend-monitoring.js
class FrontendMonitor {
  constructor(config) {
    this.config = config
    this.init()
  }
  
  init() {
    this.setupPerformanceMonitoring()
    this.setupErrorMonitoring()
    this.setupUserInteractionMonitoring()
  }
  
  // æ€§èƒ½ç›‘æ§
  setupPerformanceMonitoring() {
    // é¡µé¢åŠ è½½æ€§èƒ½
    window.addEventListener('load', () => {
      setTimeout(() => {
        const perfData = performance.getEntriesByType('navigation')[0]
        
        this.sendMetric({
          type: 'performance',
          metrics: {
            dns_lookup: perfData.domainLookupEnd - perfData.domainLookupStart,
            tcp_connect: perfData.connectEnd - perfData.connectStart,
            request_response: perfData.responseEnd - perfData.requestStart,
            dom_parse: perfData.domContentLoadedEventEnd - perfData.responseEnd,
            page_load: perfData.loadEventEnd - perfData.navigationStart
          },
          url: window.location.href,
          timestamp: Date.now()
        })
      }, 0)
    })
    
    // Core Web Vitals
    this.measureWebVitals()
  }
  
  measureWebVitals() {
    // Largest Contentful Paint (LCP)
    new PerformanceObserver((entryList) => {
      const entries = entryList.getEntries()
      const lastEntry = entries[entries.length - 1]
      
      this.sendMetric({
        type: 'web_vital',
        name: 'LCP',
        value: lastEntry.startTime,
        url: window.location.href
      })
    }).observe({ entryTypes: ['largest-contentful-paint'] })
    
    // First Input Delay (FID)
    new PerformanceObserver((entryList) => {
      const firstInput = entryList.getEntries()[0]
      
      this.sendMetric({
        type: 'web_vital',
        name: 'FID',
        value: firstInput.processingStart - firstInput.startTime,
        url: window.location.href
      })
    }).observe({ entryTypes: ['first-input'] })
    
    // Cumulative Layout Shift (CLS)
    let clsValue = 0
    new PerformanceObserver((entryList) => {
      for (const entry of entryList.getEntries()) {
        if (!entry.hadRecentInput) {
          clsValue += entry.value
        }
      }
      
      this.sendMetric({
        type: 'web_vital',
        name: 'CLS',
        value: clsValue,
        url: window.location.href
      })
    }).observe({ entryTypes: ['layout-shift'] })
  }
  
  // é”™è¯¯ç›‘æ§
  setupErrorMonitoring() {
    // JavaScript é”™è¯¯
    window.addEventListener('error', (event) => {
      this.sendError({
        type: 'javascript_error',
        message: event.message,
        filename: event.filename,
        lineno: event.lineno,
        colno: event.colno,
        stack: event.error?.stack,
        url: window.location.href,
        userAgent: navigator.userAgent,
        timestamp: Date.now()
      })
    })
    
    // Promise é”™è¯¯
    window.addEventListener('unhandledrejection', (event) => {
      this.sendError({
        type: 'promise_rejection',
        message: event.reason?.message || 'Unhandled promise rejection',
        stack: event.reason?.stack,
        url: window.location.href,
        timestamp: Date.now()
      })
    })
    
    // èµ„æºåŠ è½½é”™è¯¯
    window.addEventListener('error', (event) => {
      if (event.target !== window) {
        this.sendError({
          type: 'resource_error',
          element: event.target.tagName,
          source: event.target.src || event.target.href,
          url: window.location.href,
          timestamp: Date.now()
        })
      }
    }, true)
  }
  
  // ç”¨æˆ·äº¤äº’ç›‘æ§
  setupUserInteractionMonitoring() {
    // é¡µé¢è®¿é—®
    this.sendEvent({
      type: 'page_view',
      url: window.location.href,
      referrer: document.referrer,
      timestamp: Date.now()
    })
    
    // ç‚¹å‡»äº‹ä»¶
    document.addEventListener('click', (event) => {
      this.sendEvent({
        type: 'click',
        element: event.target.tagName,
        text: event.target.textContent?.substring(0, 100),
        url: window.location.href,
        timestamp: Date.now()
      })
    })
    
    // é¡µé¢åœç•™æ—¶é—´
    let startTime = Date.now()
    window.addEventListener('beforeunload', () => {
      this.sendEvent({
        type: 'page_duration',
        duration: Date.now() - startTime,
        url: window.location.href
      })
    })
  }
  
  sendMetric(data) {
    this.send('/api/metrics', data)
  }
  
  sendError(data) {
    this.send('/api/errors', data)
  }
  
  sendEvent(data) {
    this.send('/api/events', data)
  }
  
  send(endpoint, data) {
    if (navigator.sendBeacon) {
      navigator.sendBeacon(endpoint, JSON.stringify(data))
    } else {
      fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(data)
      }).catch(console.error)
    }
  }
}

// åˆå§‹åŒ–ç›‘æ§
const monitor = new FrontendMonitor({
  apiEndpoint: '/api/monitoring'
})
```

## å‘Šè­¦ç³»ç»Ÿ
### AlertManager é…ç½®

```yaml
# alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'password'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
  - match:
      severity: warning
    receiver: 'warning-alerts'

receivers:
- name: 'web.hook'
  webhook_configs:
  - url: 'http://webhook:5000/alerts'

- name: 'critical-alerts'
  email_configs:
  - to: 'oncall@example.com'
    subject: 'ğŸš¨ Critical Alert: {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Instance: {{ .Labels.instance }}
      Severity: {{ .Labels.severity }}
      {{ end }}
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#alerts'
    title: 'ğŸš¨ Critical Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

- name: 'warning-alerts'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#monitoring'
    title: 'âš ï¸ Warning Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
```

### è‡ªå®šä¹‰å‘Šè­¦å¤„ç†

```javascript
// alert-handler.js
const express = require('express')
const axios = require('axios')
const app = express()

app.use(express.json())

// å‘Šè­¦å¤„ç†ç«¯ç‚¹
app.post('/alerts', async (req, res) => {
  const alerts = req.body.alerts
  
  for (const alert of alerts) {
    await processAlert(alert)
  }
  
  res.status(200).send('OK')
})

async function processAlert(alert) {
  const { labels, annotations, status } = alert
  
  // æ ¹æ®å‘Šè­¦ç±»å‹å¤„ç†
  switch (labels.alertname) {
    case 'ServiceDown':
      await handleServiceDownAlert(alert)
      break
    case 'HighErrorRate':
      await handleHighErrorRateAlert(alert)
      break
    case 'DiskSpaceLow':
      await handleDiskSpaceAlert(alert)
      break
    default:
      await handleGenericAlert(alert)
  }
}

async function handleServiceDownAlert(alert) {
  // è‡ªåŠ¨é‡å¯æœåŠ¡
  try {
    await axios.post('http://orchestrator:8080/restart', {
      service: alert.labels.job,
      instance: alert.labels.instance
    })
    
    console.log(`Attempted to restart service: ${alert.labels.job}`)
  } catch (error) {
    console.error('Failed to restart service:', error.message)
  }
  
  // å‘é€ç´§æ€¥é€šçŸ¥
  await sendUrgentNotification(alert)
}

async function handleHighErrorRateAlert(alert) {
  // è§¦å‘è‡ªåŠ¨æ‰©å®¹
  await scaleService(alert.labels.job, 2)
  
  // é€šçŸ¥å¼€å‘å›¢é˜Ÿ
  await notifyDevelopmentTeam(alert)
}

async function sendUrgentNotification(alert) {
  // å‘é€çŸ­ä¿¡é€šçŸ¥
  await sendSMS({
    to: '+1234567890',
    message: `URGENT: ${alert.annotations.summary}`
  })
  
  // å‘é€é‚®ä»¶
  await sendEmail({
    to: 'oncall@example.com',
    subject: `ğŸš¨ URGENT: ${alert.labels.alertname}`,
    body: `
      Alert: ${alert.annotations.summary}
      Description: ${alert.annotations.description}
      Instance: ${alert.labels.instance}
      Time: ${new Date().toISOString()}
    `
  })
}

app.listen(5000, () => {
  console.log('Alert handler listening on port 5000')
})
```

## ç›‘æ§æœ€ä½³å®è·µ
### ç›‘æ§ç­–ç•¥

1. **åˆ†å±‚ç›‘æ§**
   - åŸºç¡€è®¾æ–½å±‚ï¼šCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ
   - åº”ç”¨å±‚ï¼šå“åº”æ—¶é—´ã€ååé‡ã€é”™è¯¯ç‡
   - ä¸šåŠ¡å±‚ï¼šç”¨æˆ·è¡Œä¸ºã€è½¬åŒ–ç‡ã€æ”¶å…¥

2. **å‘Šè­¦è®¾è®¡åŸåˆ™**
   - å¯æ“ä½œæ€§ï¼šæ¯ä¸ªå‘Šè­¦éƒ½åº”è¯¥æœ‰æ˜ç¡®çš„å¤„ç†æ­¥éª¤
   - ç›¸å…³æ€§ï¼šå‘Šè­¦åº”è¯¥ä¸å®é™…é—®é¢˜ç›¸å…³
   - åŠæ—¶æ€§ï¼šåœ¨é—®é¢˜å½±å“ç”¨æˆ·å‰å‘å‡ºå‘Šè­¦
   - é¿å…å‘Šè­¦ç–²åŠ³ï¼šå‡å°‘è¯¯æŠ¥å’Œé‡å¤å‘Šè­¦

3. **SLI/SLO è®¾è®¡**

```yaml
# SLI/SLO å®šä¹‰
service_level_objectives:
  availability:
    sli: "rate(http_requests_total{status!~'5..'}[5m]) / rate(http_requests_total[5m])"
    target: 99.9%
    window: 30d
  
  latency:
    sli: "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
    target: "< 500ms"
    window: 30d
  
  error_rate:
    sli: "rate(http_requests_total{status=~'5..'}[5m]) / rate(http_requests_total[5m])"
    target: "< 0.1%"
    window: 30d
```

## æ•…éšœæ’é™¤
### å¸¸è§é—®é¢˜

```bash
# Prometheus æ•°æ®æŸ¥è¯¢
# æŸ¥çœ‹æŒ‡æ ‡
curl http://localhost:9090/api/v1/label/__name__/values

# æŸ¥è¯¢æ•°æ®
curl 'http://localhost:9090/api/v1/query?query=up'

# æŸ¥è¯¢èŒƒå›´æ•°æ®
curl 'http://localhost:9090/api/v1/query_range?query=up&start=2023-01-01T00:00:00Z&end=2023-01-01T01:00:00Z&step=15s'

# æ£€æŸ¥å‘Šè­¦è§„åˆ™
curl http://localhost:9090/api/v1/rules

# æ£€æŸ¥ç›®æ ‡çŠ¶æ€
curl http://localhost:9090/api/v1/targets
```

### æ€§èƒ½ä¼˜åŒ–

```yaml
# prometheus.yml ä¼˜åŒ–é…ç½®
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'production'
    region: 'us-west-2'

# å­˜å‚¨ä¼˜åŒ–
storage:
  tsdb:
    retention.time: 15d
    retention.size: 50GB
    wal-compression: true

# æŸ¥è¯¢ä¼˜åŒ–
query:
  max-concurrency: 20
  timeout: 2m
  max-samples: 50000000
```

## å‚è€ƒèµ„æº
### å®˜æ–¹æ–‡æ¡£

- [Prometheus æ–‡æ¡£](https://prometheus.io/docs/)
- [Grafana æ–‡æ¡£](https://grafana.com/docs/)
- [AlertManager æ–‡æ¡£](https://prometheus.io/docs/alerting/latest/alertmanager/)
- [ELK Stack æ–‡æ¡£](https://www.elastic.co/guide/)

### å­¦ä¹ èµ„æº

- [ç›‘æ§æœ€ä½³å®è·µ](https://sre.google/sre-book/monitoring-distributed-systems/)
- [SLI/SLO æŒ‡å—](https://sre.google/sre-book/service-level-objectives/)
- [å¯è§‚æµ‹æ€§å·¥ç¨‹](https://www.oreilly.com/library/view/observability-engineering/9781492076438/)

---

> ğŸ’¡ **æç¤º**ï¼šæœ‰æ•ˆçš„ç›‘æ§ç³»ç»Ÿæ˜¯ä¿éšœæœåŠ¡ç¨³å®šæ€§çš„å…³é”®ï¼Œéœ€è¦æ ¹æ®ä¸šåŠ¡ç‰¹ç‚¹è®¾è®¡åˆé€‚çš„ç›‘æ§ç­–ç•¥å’Œå‘Šè­¦è§„åˆ™ï¼